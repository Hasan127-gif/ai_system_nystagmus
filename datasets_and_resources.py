#!/usr/bin/env python3
"""
AÃ‡IK KAYNAK VERÄ° SETLERÄ° VE KOD Ã–RNEKLERÄ°
=========================================
EÄŸitimi tekrarlanabilir yapmak ve araÅŸtÄ±rmacÄ±larÄ±n projeyi denemesi iÃ§in kaynaklar.
"""

import os
import urllib.request
import zipfile
import json
from typing import Dict, List, Any
import logging

logger = logging.getLogger(__name__)

class DatasetManager:
    """AÃ§Ä±k kaynak veri setlerini yÃ¶netir."""
    
    def __init__(self, base_path: str = "datasets"):
        self.base_path = base_path
        os.makedirs(base_path, exist_ok=True)
        
    def get_available_datasets(self) -> Dict[str, Dict[str, Any]]:
        """Mevcut aÃ§Ä±k kaynak veri setlerini listeler."""
        return {
            "UnityEyes": {
                "description": "Sentetik gÃ¶z hareketi verisi - Ã§ok Ã§eÅŸitli gÃ¶z pozisyonlarÄ±",
                "size": "~5GB",
                "format": "PNG images + JSON annotations",
                "use_case": "GÃ¶z tracking modeli eÄŸitimi",
                "github": "https://github.com/swook/UnityEyes",
                "paper": "https://arxiv.org/abs/1512.04916",
                "download_command": "git clone https://github.com/swook/UnityEyes.git",
                "license": "MIT",
                "quality": "â­â­â­â­â­",
                "samples": 1000000
            },
            "OpenEDS": {
                "description": "Facebook'un aÃ§Ä±k gÃ¶z-izleme veri seti",
                "size": "~10GB", 
                "format": "Video sequences + eye landmarks",
                "use_case": "GerÃ§ek gÃ¶z hareketi analizi",
                "github": "https://github.com/facebookresearch/openeds",
                "paper": "https://arxiv.org/abs/1905.03702",
                "download_command": "git clone https://github.com/facebookresearch/openeds.git",
                "license": "CC BY-NC 4.0",
                "quality": "â­â­â­â­â­",
                "samples": 152
            },
            "MPIIGaze": {
                "description": "GerÃ§ek gÃ¶z bakÄ±ÅŸ yÃ¶nÃ¼ veri seti",
                "size": "~3GB",
                "format": "Images + gaze directions",
                "use_case": "GÃ¶z bakÄ±ÅŸ tahmini",
                "website": "https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild/",
                "download_command": "# Ä°zin gerekir, website'den baÅŸvurun",
                "license": "Academic use only",
                "quality": "â­â­â­â­",
                "samples": 213659
            },
            "EyeTrackingData": {
                "description": "Ã‡eÅŸitli gÃ¶z tracking gÃ¶revleri veri seti",
                "size": "~1GB",
                "format": "CSV + video files", 
                "use_case": "Sakkad ve fiksasyon analizi",
                "github": "https://github.com/esdalmaijer/PyGazeAnalyser",
                "download_command": "git clone https://github.com/esdalmaijer/PyGazeAnalyser.git",
                "license": "GPL-3.0",
                "quality": "â­â­â­",
                "samples": 50000
            },
            "MediaPipe_Samples": {
                "description": "MediaPipe demo videolarÄ± ve kod Ã¶rnekleri",
                "size": "~500MB",
                "format": "MP4 videos + Python scripts",
                "use_case": "MediaPipe entegrasyonu test",
                "github": "https://github.com/google/mediapipe",
                "download_command": "git clone https://github.com/google/mediapipe.git",
                "license": "Apache 2.0",
                "quality": "â­â­â­â­â­",
                "samples": 100
            }
        }
    
    def download_sample_datasets(self):
        """Ã–rnek veri setlerini indirir."""
        print("ğŸ“¥ Ã–rnek veri setleri indiriliyor...")
        
        # 1. MediaPipe Ã¶rnek veriler
        self._download_mediapipe_samples()
        
        # 2. Nistagmus test verileri oluÅŸtur
        self._create_synthetic_nystagmus_data()
        
        print("âœ… Ã–rnek veri setleri hazÄ±r!")
    
    def _download_mediapipe_samples(self):
        """MediaPipe Ã¶rnek verilerini indir."""
        try:
            import subprocess
            
            mediapipe_path = os.path.join(self.base_path, "mediapipe_samples")
            if not os.path.exists(mediapipe_path):
                print("MediaPipe Ã¶rnekleri klonlanÄ±yor...")
                subprocess.run([
                    "git", "clone", "--depth", "1",
                    "https://github.com/google/mediapipe.git",
                    mediapipe_path
                ], check=True)
                print("âœ… MediaPipe Ã¶rnekleri indirildi")
            else:
                print("â„¹ï¸ MediaPipe Ã¶rnekleri zaten mevcut")
                
        except Exception as e:
            logger.error(f"MediaPipe Ã¶rnekleri indirme hatasÄ±: {e}")
    
    def _create_synthetic_nystagmus_data(self):
        """Sentetik nistagmus verisi oluÅŸtur."""
        try:
            import cv2
            import numpy as np
            
            synthetic_path = os.path.join(self.base_path, "synthetic_nystagmus")
            os.makedirs(synthetic_path, exist_ok=True)
            
            # FarklÄ± nistagmus tiplerinde test videolarÄ± oluÅŸtur
            nystagmus_types = {
                "normal": {"freq": 0.5, "amplitude": 5},
                "mild_nystagmus": {"freq": 3.0, "amplitude": 15},
                "severe_nystagmus": {"freq": 8.0, "amplitude": 30},
                "strabismus": {"freq": 1.0, "amplitude": 10, "offset": 20}
            }
            
            for nys_type, params in nystagmus_types.items():
                video_path = os.path.join(synthetic_path, f"{nys_type}.mp4")
                
                if not os.path.exists(video_path):
                    print(f"ğŸ¥ {nys_type} videosu oluÅŸturuluyor...")
                    self._create_nystagmus_video(video_path, **params)
            
            # Metadata oluÅŸtur
            metadata = {
                "dataset_info": {
                    "name": "Synthetic Nystagmus Dataset",
                    "version": "1.0",
                    "created_by": "Nystagmus Detection System",
                    "samples": len(nystagmus_types),
                    "format": "MP4 videos with simulated eye movements"
                },
                "samples": nystagmus_types
            }
            
            with open(os.path.join(synthetic_path, "metadata.json"), 'w') as f:
                json.dump(metadata, f, indent=2)
            
            print("âœ… Sentetik nistagmus verisi oluÅŸturuldu")
            
        except Exception as e:
            logger.error(f"Sentetik veri oluÅŸturma hatasÄ±: {e}")
    
    def _create_nystagmus_video(self, output_path: str, freq: float, amplitude: float, offset: int = 0):
        """Belirli parametrelerle nistagmus videosu oluÅŸtur."""
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, 30.0, (640, 480))
        
        for i in range(300):  # 10 saniye
            frame = np.zeros((480, 640, 3), dtype=np.uint8)
            
            # YÃ¼z Ã§iz
            cv2.circle(frame, (320, 240), 100, (200, 150, 100), -1)
            
            # Nistagmus hareketi simÃ¼lasyonu
            left_x = 300 + int(amplitude * np.sin(i * freq * 0.2))
            right_x = 340 + int(amplitude * np.sin(i * freq * 0.2)) + offset
            
            # GÃ¶zleri Ã§iz
            cv2.circle(frame, (left_x, 220), 12, (255, 255, 255), -1)
            cv2.circle(frame, (right_x, 220), 12, (255, 255, 255), -1)
            cv2.circle(frame, (left_x, 220), 5, (0, 0, 0), -1)
            cv2.circle(frame, (right_x, 220), 5, (0, 0, 0), -1)
            
            out.write(frame)
        
        out.release()

def get_code_examples() -> Dict[str, str]:
    """KullanÄ±ÅŸlÄ± kod Ã¶rnekleri dÃ¶ndÃ¼rÃ¼r."""
    return {
        "mediapipe_installation": """
# MediaPipe kurulumu
pip install mediapipe opencv-python

# Veya kaynak koddan
git clone https://github.com/google/mediapipe.git
cd mediapipe
python setup.py install
        """,
        
        "advanced_eye_tracking": """
# GeliÅŸmiÅŸ gÃ¶z tracking iÃ§in ConVNG
pip install git+https://github.com/guanfuchen/ConVNG.git

# Veya manuel kurulum
git clone https://github.com/guanfuchen/ConVNG.git
cd ConVNG
pip install -r requirements.txt
python setup.py install
        """,
        
        "performance_profiling": """
import time
import cv2
from detector import NystagmusDetector

def profile_detection_speed():
    detector = NystagmusDetector()
    
    # Test videosu yÃ¼kle
    cap = cv2.VideoCapture('test_video.mp4')
    frame_count = 0
    start_time = time.time()
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
            
        # Iris tespit et
        detector.detect_iris_centers(frame)
        frame_count += 1
        
        if frame_count >= 100:  # 100 kare test
            break
    
    end_time = time.time()
    fps = frame_count / (end_time - start_time)
    
    print(f"FPS: {fps:.2f}")
    print(f"Frame baÅŸÄ±na sÃ¼re: {1000/fps:.2f}ms")
    
    cap.release()
    return fps

# Performans testi Ã§alÄ±ÅŸtÄ±r
if __name__ == "__main__":
    fps = profile_detection_speed()
    if fps >= 25:
        print("âœ… GerÃ§ek zamanlÄ± kullanÄ±m iÃ§in uygun")
    else:
        print("âš ï¸ Optimizasyon gerekli")
        """,
        
        "dataset_preprocessing": """
import os
import json
import cv2
import numpy as np

def preprocess_eye_tracking_dataset(dataset_path):
    \"\"\"GÃ¶z tracking veri setini Ã¶n iÅŸle.\"\"\"
    
    processed_data = []
    
    for video_file in os.listdir(dataset_path):
        if video_file.endswith('.mp4'):
            video_path = os.path.join(dataset_path, video_file)
            
            # Video yÃ¼kle
            cap = cv2.VideoCapture(video_path)
            
            frame_data = []
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # GÃ¶z tespiti yap
                # (Burada kendi detector'Ä±nÄ±zÄ± kullanÄ±n)
                
                frame_data.append({
                    "timestamp": cap.get(cv2.CAP_PROP_POS_MSEC),
                    "features": "extracted_features"
                })
            
            cap.release()
            
            processed_data.append({
                "video": video_file,
                "frames": frame_data
            })
    
    # SonuÃ§larÄ± kaydet
    with open('processed_dataset.json', 'w') as f:
        json.dump(processed_data, f, indent=2)
    
    return processed_data
        """,
        
        "data_augmentation": """
import cv2
import numpy as np
from typing import Tuple

def augment_eye_video(video_path: str, output_path: str):
    \"\"\"GÃ¶z videosunu Ã§eÅŸitli augmentasyonlarla Ã§oÄŸalt.\"\"\"
    
    cap = cv2.VideoCapture(video_path)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    # FarklÄ± augmentasyon tÃ¼rleri
    augmentations = [
        ("brightness", lambda x: cv2.convertScaleAbs(x, alpha=1.2, beta=30)),
        ("contrast", lambda x: cv2.convertScaleAbs(x, alpha=1.5, beta=0)),
        ("blur", lambda x: cv2.GaussianBlur(x, (3, 3), 0)),
        ("noise", lambda x: add_noise(x))
    ]
    
    for aug_name, aug_func in augmentations:
        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # BaÅŸa sar
        
        out_path = output_path.replace('.mp4', f'_{aug_name}.mp4')
        out = cv2.VideoWriter(out_path, fourcc, fps, (640, 480))
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            augmented_frame = aug_func(frame)
            out.write(augmented_frame)
        
        out.release()
        print(f"âœ… {aug_name} augmentasyonu tamamlandÄ±: {out_path}")
    
    cap.release()

def add_noise(image: np.ndarray) -> np.ndarray:
    \"\"\"GÃ¶rÃ¼ntÃ¼ye gÃ¼rÃ¼ltÃ¼ ekle.\"\"\"
    noise = np.random.normal(0, 25, image.shape).astype(np.uint8)
    return cv2.add(image, noise)
        """
    }

def print_dataset_installation_guide():
    """Veri seti kurulum rehberi yazdÄ±r."""
    
    print("=" * 80)
    print("ğŸ“Š AÃ‡IK KAYNAK VERÄ° SETLERÄ° KURULUM REHBERÄ°")
    print("=" * 80)
    
    datasets = DatasetManager().get_available_datasets()
    
    for name, info in datasets.items():
        print(f"\nğŸ¯ {name}")
        print(f"   ğŸ“ AÃ§Ä±klama: {info['description']}")
        print(f"   ğŸ’¾ Boyut: {info['size']}")
        print(f"   ğŸ“ Format: {info['format']}")
        print(f"   ğŸ¯ KullanÄ±m: {info['use_case']}")
        print(f"   â­ Kalite: {info['quality']}")
        print(f"   ğŸ“Š Ã–rnek sayÄ±sÄ±: {info['samples']}")
        print(f"   ğŸ“„ Lisans: {info['license']}")
        print(f"   ğŸ“¥ Ä°ndirme:")
        print(f"      {info['download_command']}")
        
        if 'github' in info:
            print(f"   ğŸ”— GitHub: {info['github']}")
        if 'paper' in info:
            print(f"   ğŸ“š Makale: {info['paper']}")
        if 'website' in info:
            print(f"   ğŸŒ Website: {info['website']}")
        
        print("-" * 50)

if __name__ == "__main__":
    print_dataset_installation_guide()
    
    # Ã–rnek veri seti yÃ¶neticisi
    manager = DatasetManager()
    manager.download_sample_datasets() 